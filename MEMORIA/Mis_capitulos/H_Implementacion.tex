\chapter{Implementación} \label{implementacion}
\section{Prueba de concepto} \label{implementacion.prueba}

En la fase inicial del proyecto lo primero que se hizo fue demostrar si las herramientas elegidas en el stack tecnológico son viables, si funcionan en conjunto, determinar los problemas que presentan y los retos a los que se enfrenta desde una aproximación tecnológica. Conceptualmente, este apartado podría verse englobado dentro de la sección de Análisis (Capítulo \ref{analisis}) ya que la prueba de concepto fue la que determinó el \textit{Stack Tecnológico}. No obstante, dado que realmente formó parte del desarrollo de la solución se ha decidido redactarlo como una primera parte del apartado de Implementación de la solución. Así pues, en el diagrama de la figura \ref{fig:pruebaconceptoglobal} se puede observar el panorama global de los pasos que se han dado para montar toda la infraestructura necesaria para el proyecto en cuanto a herramientas tecnológicas se refiere. 

\begin{landscape}

\begin{figure}[p!]
    
    \includegraphics[width=\linewidth]{Imagenes/pruebadeconceptoglobal}
    \caption{Diagrama de las etapas de la prueba de concepto.}
    \label{fig:pruebaconceptoglobal}
\end{figure}

\end{landscape}


\par

\textbf{Pasos reflejados en el diagrama de la figura \ref{fig:pruebaconceptoglobal}:}
\begin{itemize}
\item \textbf{1º.} Instalación y configuración de \textit{Apache Hadoop}.
\item \textbf{2º.} Instalación y configuración de \textit{Apache Hive}.
\item \textbf{3º.} Configuración de la conexión de \textit{Apache Hadoop} con \textit{Apache Hive}.
\item \textbf{4º.} Instalación y configuración de \textit{JHipster}.
\item \textbf{5º Fallido.} Intento fallido de conexión directa entre \textit{JHipster} y \textit{Apache Hive}.
\item \textbf{6º.} Instalación y configuración de \textit{Apache Sqoop}. 
\item \textbf{7º.} Exportación de datos desde \textit{Apache Hive} a \textit{MySQL} (base de datos de \textit{JHipster}) con \textit{Apache Sqoop}.
\item \textbf{8º.} Instalación y configuración de \textit{Kettle Pentaho}.
\item \textbf{9º.} Desarrollo de procesos \textit{ETL} para \textit{Apache Hadoop} mediante \textit{Kettle Pentaho}.
\item \textbf{10º Fallido.} Intento fallido de importación de procesos de \textit{Kettle Pentaho} en \textit{JHipster}.
\item \textbf{11º.} Instalación y configuración de\textit{ Talend Big Data}.
\item \textbf{12º.} Desarrollo de procesos \textit{ETL} para \textit{Apache Hadoop }mediante \textit{Talend Big Data}.
\item \textbf{13º.} Importación de procesos de \textit{Talend Big Data} en \textit{JHipster}.
\end{itemize}

A continuación se van a presentar detalladamente las herramientas usadas y las pruebas realizadas reparándo sólamente en aquellos pasos más conflictivos, y se van a exponer algunos de los retos enfrentados durante la prueba de concepto: 
\bigskip
\par
\paragraph*{Hadoop.}

\par
Inicialmente la instalación de \textit{Hadoop} supuso algunos problemas puesto que el alumno no había tenido contacto con la herramienta previamente, y la \textit{información} \cite{hadoop_installation_bad} que el alumno seleccionó como base para la instalación estaba desafortunadamente incorrecta (la instalación disponible en dicha web había sido probada con Ubuntu Linux 10.04, pero no con la versión del alumno, la 16.04). Por lo tanto, se tuvo que empezar de cero, eliminando cualquier rastro de la primera instalación de \textit{Hadoop} del sistema. Después de ello, en un segundo intento, y gracias al \textit{tutorial de instalación de Hadoop de Digital Ocean} \cite{hadoop_installation} el programa funcionó correctamente y se procedió a instalar el siguiente bloque software necesario para el funcionamiento del sistema: \textit{Hive}.

\bigskip
\par
\paragraph*{Hive.}

\par
Para la instalación de \textit{Hive} ocurrió un problema similar al de \textit{Hadoop}. La fuente elegida para su instalación no fue la adecuada en un principio; el alumno eligió el tutorial expuesto en la web \textit{Tutorial's Point} \cite{hivetutorialspoint}, que provee información no solo excesiva sino en ocasiones confusa. Como en el caso anterior, se tuvo que erradicar \textit{Hive} del sistema para proceder con una instalación más limpia, esta vez desde la \textit{página web oficial de Hive} \cite{hive_installation}, puesto que lo único requerido para su instalación fue su descarga y la declaración de las variables de entorno necesarias para su ejecución. De esta manera, se consiguió instalar la versión 2.2.1 de \textit{Hive} sin dificultades. 

\bigskip
\par
\paragraph*{JHipster: Instalación.}

\par
Lo siguiente que se probó fue a instalar \textit{JHipster}. Como \textit{Java} y \textit{Node.js} \cite{nodejs}, dos de los componentes necesarios para su instalación ya estaban configurados en el equipo, lo único que se tuvo que configurar fue \textit{Yarn}, que se hizo siguiendo los pasos recomendados para \textit{Linux} en la \textit{página oficial de Yarn} \cite{yarninstall} para poder instalar \textit{JHipster} con el comando \textit{yarn global add generator-jhipster}, tal como indica la página oficial de \textit{JHipster}. Esto en sí fue facil, y no supuso mayor problema; No obstante, el desconocimiento de \textit{Yarn} junto con las actualizaciones periódicas que se introducían en \textit{JHipster} hicieron que más de una vez se tuviese que borrar \textit{JHipster} del equipo y volver a instalarlo en su última versión.  

\bigskip
\par
\paragraph*{JHipster: Aplicación de prueba.}

\par
Con \textit{JHipster} instalado y configurado en el sistema, el siguiente paso obvio fue crear una aplicación y verla en funcionamiento. Para ello se siguió el tutorial del que se provee en la \textit{página oficial} \cite{jhipster}. La creación de la aplicación con JHipster resultó bastante sencillo puesto que se trata de pasos secuenciales. No obstante, dado que en la web no existe mucha información acerca de la integración de un proyecto  \textit{JHipster} con \textit{IntelliJ} \cite{intellij} (entorno de desarrollo usado por el alumno), el arranque resultó bastante frustrante. El poco dominio que el alumno tenía tanto de \textit{Gradle} como del propio entorno supuso un reto en las fases tempranas del proyecto, que se superó a base de leer documentación y realizar diversos intentos hasta que por fin se consiguió una versión de la aplicación corriendo en local, en el puerto 8080. 

\bigskip
\par
\paragraph*{Integración Hadoop - Hive - JHipster.}

\par
Si bien es cierto que la integración entre \textit{Hive} y \textit{Hadoop} resulta casi trivial, la integración entre \textit{Hive} y \textit{JHipster} es todo lo contrario. En primer lugar, se quería conectar \textit{Hive} con \textit{Hadoop} para disponer de una ayuda relacional para poder hacer consultas sobre datos en formatos no relacionales. Su configuración se realizó modificando los ficheros de configuración dentro del directorio de instalación de \textit{Hive}, para permitir una conexión entre los servicios de \textit{Hive} y los nodos de \textit{Hadoop}. En segundo lugar, la conexión entre \textit{Hive} y \textit{JHipster} se quería realizar para tener un flujo directo entre la información que se muestra en pantalla desde \textit{JHipster} y los datos que son importados en \textit{Hadoop} desde las diferentes fuentes. Para ello hay que tener en cuenta que cuando se crea una aplicación con \textit{JHipster}, este pregunta por la base de datos que se quiera usar tanto en producción como en desarrollo. Actualmente \textit{JHipster} ofrece soporte para las siguientes bases de datos: \textit{\gls{mongodb}}, \textit{\gls{cassandra}}, o una base de datos SQL (\textit{\gls{h2}}, \textit{\gls{mysql}}, \textit{\gls{mariadb}}, \textit{\gls{postgresql}}, \textit{\gls{mssql}}, \textit{\gls{oracle}}).
Como se puede observar, \textit{JHipster} no ofrece soporte oficial para una base de datos correspondiente ni a \textit{Hive}, ni a \textit{Hadoop}. Por eso, inicialmente la aplicación de \textit{JHipster} se creó con una base de datos relacional \textit{MySQL} puesto que su sintáxis es la más parecida a la de \textit{Hive} (aunque no es igual). El objetivo en este caso fue conectar \textit{JHipster} con \textit{Hive} diréctamente, sustituyendo de alguna manera la base de datos \textit{MySQL} y cambiando cualquier interacción que se tuviera con ella. No obstante, las tablas que se crean durante la creación de la aplicación se crean con una sintáxis propia de \textit{MySQL}, en la base de datos \textit{MySQL} y con un esquema a priori no visible. Tras muchas horas invertidas, muchos portales consultados, muchas preguntas en diversos foros de Internet, esta tarea se marcó como inalcanzable y se procedió a buscar otras soluciones con una viabilidad más alta. 


\bigskip
\par
\paragraph*{Sqoop.}
\par
Visto el resultado de la prueba anterior y por lo tanto el abandono de ese camino, el paso más lógico que se debía tomar a continuación era añadir un componente intermedio capaz de transferir datos de \textit{Hadoop/Hive} a \textit{MySQL}. Afortunadamente, ese componente existe y se trata de la herramienta \textit{Sqoop}, que permite transferir datos de una tabla de \textit{Hive} a otra tabla de una base de datos relacional, con un formato parecido o igual a la de \textit{Hive}. Así pues, gracias a \textit{Sqoop} conseguíamos disponer del mecanismo mediante el que los datos importados en \textit{Hadoop} podían ser visualizados casi diréctamente (con su previa carga en \textit{Hive}) en el \textit{Front-End} provisto por \textit{JHipster}. 


\bigskip
\par
\paragraph*{Estado de la prueba de concepto.}

\par
Con \textit{Hive} conectado a \textit{Hadoop}, \textit{Sqoop} en marcha y una aplicación \textit{JHipster} de prueba para ver un primer resultado de la integración de los datos, el sistema funcionaba acorde a las espectativas del \textit{workflow} de la información: Almacen de los datos en \textit{Hadoop} tanto en su versión ``en crudo'' como en una versión procesada, recuperación de los datos procesados desde \textit{Hive}, transferencia de los mismos a la base de datos \textit{MySQL} mediante \textit{Sqoop} y visualización por pantalla mediante la aplicación de \textit{JHipster}. No obstante, analizando el estado de la prueba de concepto, se podía observar que en el anterior proceso faltaban tres cosas: 
\begin{itemize}
\item \textbf{Automatización} de las tareas involucradas: Hasta este punto, cualquier parte del proceso requería de la intervención manual de un usuario, esto es, descarga de los datos desde sus fuentes, procesado de los mismos, carga de la información en \textit{Hadoop}, creación de una tabla en \textit{Hive} correspondiente a los datos de dicha fuente particular, carga de los datos desde \textit{Hadoop} a \textit{Hive} y la transferencia de los mismos mediante \textit{Sqoop} hacia la base de datos de \textit{JHipster}, \textit{MySQL}. Viene siendo evidente la necesidad de un mecanismo que permita la automatización de todas estas tareas, con una intervención mínima por parte de un usuario. Esto permitiría aparte de un incremento considerable en el tiempo de resolución del \textit{workflow}, un desarrollo futuro más ágil y sencillo. 
\item \textbf{Procesado de los datos} de manera eficaz: El \textit{TFG} requería de un módulo de procesado de datos puesto que, como ya se ha explicado en ocasiones durante esta memoria, los datos pueden provenir de diferentes fuentes en formatos heterogéneos. Así pues, una carencia en este punto era esa herramienta o módulo que permitiera trabajar con datos en distintos formatos de una manera rapida y eficaz. Previamente los datos se habían ``procesado'' manualmente, con un sencillo editor de texto. 
\item \textbf{Actualización periódica} de la ejecución de todas las tareas: Teniendo en mente una visión futura y acabada del proyecto, otro aspecto que se echaba en falta en este punto era la posibilidad de que todo el \textit{workflow} se ejecutase de manera periódica, obteniendo con esto una gran ventaja: la de proveer al consumidor de unos datos actualizados en todo momento. 
\end{itemize}


\bigskip
\par
\paragraph*{Kettle.}

\par
Dejándo de lado la \textit{automatización de las tareas involucradas} y \textit{la actualización periódica de la ejecución de todas las tareas}, lo siguiente que se abordó durante la prueba de concepto fue el problema del \textit{procesado de los datos}. Para ello se requería del uso de alguna herramienta capaz de conectarse con \textit{Hive} o con \textit{Hadoop}, cuya especialidad sean las operaciones \textit{ETL}. El director del proyecto impuso para esto la herramienta \textit{Pentaho - Kettle} \cite{kettle} dada su previa experiencia con este tipo de programas, sobretodo en el área \textit{``GEO''}. Así pues, dentro del abanico de los diferentes productos de \textit{Pentaho} \cite{pentaho} se encontraba \textit{Pentaho Data Integration}, también conocida como \textit{Kettle}, herramienta libre y gratuita con un diseñador gráfico para realizar operaciones \textit{ETL} que, según mencionaba, permitía una integración sencilla con diferentes tecnologías como \textit{Hive} y \textit{Hadoop}, e incluso ofrecía soporte para una integración con proyectos \textit{Java}. Según las especificaciones del producto, parecía que encajaba perfectamente con las necesidades del proyecto. No obstante, resultó en un dolor de cabeza constante desde el momento de su instalación. No solo la interfaz del programa presentaba fallos, mezclando módulos en español con módulos en inglés o duplicando algunas funcionalidades, sino que además, el intento de migrar los procesos construidos en \textit{Pentaho Data Integration} a la aplicación de \textit{JHipster} resultaron en muchas horas de frustración, errores y hasta largas tutorías con el director del proyecto para intentar portar el código. Tras muchos días o incluso semanas de intentos y diversas formas de abordar el problema, lo que realmente acabó por apartar \textit{Kettle} del \textit{Stack Tecnológico} fue la adquisición de \textit{Pentaho} por parte de \textit{Hitachi} \cite{hitachi}, privatizando el producto bajo el nombre de \textit{Hitachi Vantara} \cite{hitachivantara} y ofreciéndo sólamente una versión de prueba del mismo. 


\bigskip
\par
\paragraph*{Talend.}

\par
Tras la privatización de \textit{Kettle} y las tantas horas dedicadas a su integración dentro del proyecto de \textit{JHipster}, se descartó \textit{Pentaho} como parte del \textit{Stack Tecnológico} y se empezó a buscar otras alternativas. La primera herramienta explorada fue \textit{KNIME} \cite{knime} por recomendación de un compañero. Tras hacer algunas pruebas rápidas, se descubrió que realmente, aunque se ofertase como herramienta libre y gratuita, que lo era, algunas de sus funcionalidades eran de pago. La siguiente opción explorada fue \textit{Talend} \cite{talend}, que resultó ser la pieza clave para el funcionamiento del proyecto gracias a la sencillez de sus componentes, a la efectividad de su editor gráfico y gracias a una documentación extensa y bien organizada. A diferencia de las otras opciones para el procesado de los ficheros, con \textit{Talend} se consiguió realizar una demostración de su funcionamiento mediante un sencillo proceso integrado en un proyecto \textit{Java} nuevo, totalmente funcional. Ese proyecto posteriormente se empaquetó en un \textit{.jar} y se exportó al proyecto de \textit{JHipster} desde el que se pudo ejecutar con éxito, sin ningún problema de compatibilidad con el código ya existente.  


\bigskip
\par
\paragraph*{Conclusiones.}

\par
Una vez conseguidos los pilares fundamentales de la integración dentro del proyecto (\textit{Hadoop}, \textit{Hive}, \textit{JHipster}, \textit{Sqoop}, \textit{Talend}) realmente las únicas preocupaciones que quedaban eran la automatización íntegra del proceso y la ejecución periódica del mismo para disponer de los datos en su versión actualizada. No obstante, para estas tareas no fue necesaria una prueba de concepto puesto que todo esto se podía conseguir desde el propio proyecto de \textit{JHipster}, mediante \textit{Spring} y código \textit{Java}, cosas con las que el alumno ya estaba familiarizado. Dándo por finalizada la prueba de concepto, se empezó a diseñar y construir el prototipo real que quedaría como solución real del proyecto.


\section{Prototipo real} \label{implementacion.prototipo}
\par 
\textbf{Primera iteración para conseguir una integración y automatización completa - Productos fitosanitarios autorizados de España}
\bigskip
\par Lo primero que se hizo entrando en el desarrollo del prototipo real fue implementar un simple proceso mediante la interfaz gráfica de Talend. Este proceso realiza las siguientes operaciones: 
\begin{enumerate}
\item Descarga desde la web del \textit{Mapama} el fichero excel de los productos fitosanitarios autorizados.
\item Convierte dicho excel a un formato openoffice para poder ser procesado desde Talend con los componentes excel correspondientes. 
\item Sube a Hadoop una versión sin procesar del fichero
\item Procesa el fichero añadiendole una columna llamada ID al principio y lo sube como versión procesada a Hadoop. 
\end{enumerate} 

A continuación se exportó el proceso desde Talend: 
\textit{Archivo $\rightarrow$ Export $\rightarrow$ Java $\rightarrow$ JAR file}. Esto exporta las clases y librerias que Talend necesita para lanzar el job en un archivo comprimido llamado <nombre\_job>.jar
El siguiente paso fue descomprimir el JAR en cuestión, analizar su contenido y ver cómo se podría importar en un proyecto Java. El JAR contenía varias carpetas y ficheros pero lo que interesa es lo siguiente:
\bigskip
\par 
\dirtree{%
.1 Nombre\_del\_jar.
.2 lib.
.3 \textit{librerias jar}.
.2 Nombre\_del\_proyecto.
.3 Nombre\_del\_job.
.4 \textit{clase java principal del job}.
.2 routines.
.3 system.
.4 api.
.5 \textit{clases java}.
.4 xml.
.5 sax.
.6 \textit{clases java}.
.4 \textit{clases java}.
.3 \textit{clases java}.
}
\bigskip
\par
Así pues, a continuación se creó un nuevo proyecto Java con IntelliJ y Maven (TalendCrawler) y se copiaron todas las clases Java con su correspondiente estructura de carpetas. Dentro del fichero pom.xml del proyecto TalendCrawler donde se importaron todas las dependencias de Talend que figuraban como librerías locales en la carpeta lib. Para ello se tuvo que definir el \textit{repositorio de Cloudera} \cite{cloudera}, que es desde donde Maven buscaría la mayoría de librerías. Tras comprobar que la aplicación arrancaba y se comportaba correctamente, el próximo paso fue encapsular y exportar la aplicación como un Jar, en conjunto con sus librerías. Para ello se hizo uso del plugin \textit{one-jar} de Maven que recoge las dependencias del proyecto y las empaqueta junto a las otras clases en un único jar.\par 

En el proyecto de JHipster lo que se hizo fue crear una clase llamada Talend, desde la que periódicamente (mediante @Scheduled) se ejecutaba el Jar anterior a través del comando Runnable. 

\par
Teniendo ya el proceso de \textit{Talend} integrado en la aplicación de \textit{JHipster}, el siguiente problema a abordar fue el de la automatización de su ejecución. Se sabe que los productos fitosanitarios autorizados son actualizados periódicamente en la web de \textit{Mapama}. Por eso mismo, nuestra aplicación requería también de una descarga periódica de dichos datos, para asegurarse de que en todo momento el programa tiene la versión actualizada de los fitosanitarios autorizados de España. Esto se consiguió gracias al \textit{módulo de scheduling}\cite{spring_scheduling} de \textit{Spring} que permite programar la ejecución de un método de manera periódica. Como decisión estratégica se propuso lanzar el proceso de \textit{Talend} cada media hora. Resuelto este problema también, el siguiente objetivo fue automatizar toda la ejecución del proceso, desde la descarga del fichero de los productos autorizados hasta la visualización de los datos mediante \textit{JHipster}. Aprovechandose del mismo módulo anterior de scheduling, el desarrollo tendría que seguir el siguiente esquema: 
\begin{itemize}
\item Primero, los datos deberían descargarse y procesarse y almacenarse en \textit{Hadoop} mediante el módulo de \textit{Talend}.
\item A continuación, se debería implementar otro módulo encargado de la carga de dichos datos procesados a una tabla de \textit{Hive}.
\item Después de eso, se deberían transferir los datos de \textit{Hive} a la base de datos \textit{MySQL} que emplea \textit{JHipster}.
\end{itemize}
  \par Así pues, para cada uno de los módulos mencionados se creó un paquete con una clase que contenía los métodos necesarios para lograr sus tareas particulares.


\par 
\textbf{Segunda iteración para conseguir una integración y automatización completa - Sustancias activas de Europa}
\bigskip
\par 
La primera iteración supuso los mayores problemas debido no solo al desconocimiento previo de las tecnologías sino también al hecho de no saber exactamente si dichas tecnologías iban a funcionar en conjunto. Una vez conocidas las tecnologías y tomado un primer contacto con ellas (el alumno no había trabajado con \textit{Talend} previamente) la segunda parte de la integración se llevó a cabo de una manera mucho más fluída. Para esta iteración se conocía previamente el \textit{modus operandi} para automatizar todo el proceso, desde la descarga de los datos hasta su visualización con \textit{JHipster}. Por lo tanto, lo único diferente con respecto a la primera iteración fue desarrollar el trabajo de procesado específico de los datos de entrada. 
\par  
Para la segunda iteración se eligieron los datos expuestos en la \textit{Base de datos europea sobre pesticidas} \cite{pesticides_eu} para seguir expandiendo la solución. Como ya se ha explicado, el objetivo de este proyecto es conseguir validar un modelo de integración para datos sobre productos fitosanitarios. En la primera iteración se obtuvieron los datos sobre los productos fitosanitarios autorizados en España. Estos contenían un campo llamado \textit{Formulado}. Dicho campo se refiere a la \textit{sustancia activa} de cada producto. Resulta que los datos descargados de la \textit{base de datos europea} contienen una amplia estructura de datos e información relativa a los productos fitosanitarios. No obstante, dicha cantidad de información también resulta excesiva. Por ello, se ha optado por una aproximación minuciosa, cogiendo y procesando un solo elemento de todos los disponibles a la vez. En este caso dicho elemento corresponde a un fichero con la información relativa a las \textit{sustancias activas}. Esta aproximación permitire ese objetivo de integración puesto que gracias a ello se puedo hacer un mapeo casi directo con los datos sobre productos autorizados de España. 
\par 
De igual manera que en la primera iteración, se implementó en \textit{Talend} el workflow necesario para procesar los datos de las sustancias activas. Esto es, por una parte, descargarlos de la página web, añadir la fecha y hora del momento de la descarga y guardarlos en \textit{Hadoop} como datos en crudo de España sin alterar ni su formato ni su contenido. Por otra parte se formateó el contenido, para almacenar en \textit{Hadoop} un fichero \textit{.csv} con sólamente la información relevante del fichero original y con una columna extra para el identificador de las filas. El mismo proceso de Talend también se encarga de subir este \textit{.csv} a Hadoop en la carpeta de datos procesados de Europa. 
\par
A continuación se preparó la infraestructura necesaria para soportar la carga de datos en \textit{Hive} mediante una nueva tabla que se mantendrá actualizada con los datos más recientes sobre sustancias activas de Europa. Esto se consiguió gracias al desarrollo implementado en el proyecto de \textit{JHipster} desde el que periódicamente se lanza el workflow anterior de Talend, y posteriormente se realiza una importación de los datos a \textit{Hive}. Además, en el lado del cliente, en \textit{JHipster} se creó la tabla correspondiente a la d \textit{Hive} en \textit{MySQL} y, una vez más, periódicamente, los datos de \textit{Hive} son transferidos a la base de datos \textit{MySQL} a través de \textit{Sqoop}. El resultado de esta iteración es que periódicamente, en \textit{JHipster} se pueden visualizar los datos actualizados de las sustancias activas europeas sin necesidad de que el usuario tenga que intervenir o interactuar con el sistema en ningún momento. 


\bigskip

\par 
\textbf{Tercera iteración para conseguir una integración y automatización completa - unión de los datos anteriores en una nueva tabla - Fitosanitario\_Sustancia\_Activa\_Europa}
\bigskip
\par 
Mientras que las dos primeras iteraciones se centraron en recoger datos periódicamente de fuentes independientes, subirlas a \textit{Hadoop} y luego importarlas en \textit{Hive} y \textit{MySQL} para ser consumidas por \textit{JHipster}, la tercera iteración tuvo que ver con la integración de dichas fuentes independientes dentro del sistema. Como se ha mencionado anteriormente, los datos de las sustancias activas europeas se eligieron como fuente para este proyecto dado que encajaban en cierta medida con los datos de los productos fitosanitarios autorizados en España:  Estos ultimos contienen un campo referente a las sustancias activas involucradas en el producto autorizado y gracias a eso se pudo hacer un \textit{mapping} entre ellos. No obstante, el \textit{mapping} no fue directo, puesto que los datos no venían en el mismo formato: en el caso de los productos autorizados, el campo en cuestión contenía además de los nombres de las sustancias activas en mayúscula la cantidad en la que podian estar presentes, mientras que en el caso de las sustancias activas europeas, los nombres venían en minúscula y sin la cantidad correspondiente. Así pues, en una primera aproximación lo que se hizo fue crear una tabla que contuviera los datos de los productos autorizados de España más una columna que fuera el identificador real de la \textbf{primera} sustancia activa involucrada en el producto. \par Esta aproximación no es la solución perfecta, no obstante, es una primera iteración que soluciona una parte del problema. Se consiguió gracias a una consulta en \textit{Hive} que partía los datos del campo "formulado" (referente a las sustancias activas que forman el producto) de los productos autorizados de España, se quedaba con la primera cadena de sólamente literales y hacía el \textit{JOIN} con el nombre de la sustancia activa (pasado a mayúsculas) de la tabla de las sustancias activas europeas. 
\par Como primera solución provisional, se consigue hacer un \textit{matching} exitoso de unos cuatrocientos registros de un total de aproximadamente mil trescientas sustancias activas. Los problemas que presenta son los siguientes: 


\begin{itemize}
\item Hay productos autorizados que tienen mas de una sustancia activa como parte de su formulado y la consulta solo reconoce la primera de ellas.
\item Hay sustancias activas que aparecen en los productos autorizados de España que vienen en español y la consulta no es capaz de reconocerlos puesto que las sustancias activas de europa tienen su nomenclatura en inglés.
\end{itemize}


\bigskip

\par 
\textbf{Fichero de configuración}
\par
Para simplificar el acceso a los recursos se ha hecho uso de un fichero de configuración a los que acceden varios componentes: En primer lugar, el script bash que descarga los datos de los productos autorizados del \textit{Mapama} \cite{mapama}. Este Script usa una función bash para solicitar los valores del fichero de propiedades de la web del \textit{Mapama}, y saber la ruta en el sistema donde guardar dicho fichero. Si en cualquier momento se quiere modificar dicha localización, gracias al fichero de configuración, el único sitio que se debería modificar sería en el propio fichero. 
\par En segundo lugar, la aplicación Java del Job de Talend también accede a dicho fichero de configuración, puesto que en él se han establecido tanto rutas de almacenamiento dentro del HDFS de Hadoop, como el nombre del nodo o del usuario. No obstante, tal como se ha comentado en el apartado anterior, esta aplicación Java ha tenido que ser empaquetada en un Jar único y conjunto con todas sus librerías. Entonces ... ¿cómo accede a dicho fichero de configuración?. La solución ha sido hacer que el Jar reciba la ruta a dicho fichero mediante un argumento, de forma robusta, tal que si no recibe argumentos, o si el fichero que se le pasa no es un fichero de propiedades, el proceso alerta del error y se detiene. 
\bigskip


\section{Problemas encontrados} \label{implementacion.problemas}
\par
Tal como se ha explicado a lo largo del desarrollo de esta memoria, tanto en la fase de la prueba de concepto como en la fase de desarrollo del prototipo se han encontrado diversos problemas de naturaleza técnica o tecnológica:

\begin{itemize}
\item \textbf{Incertidumbre inicial.} El primero de los problemas que se detectaron tienen que ver con el arranque del proyecto. Debido a una incertidumbre inicial en cuanto a la estructura de su desarollo, en el arranque del proyecto no se pudo realizar una planificación inicial para dejar definida una visión global de toda su duración. Por ello surgieron problemas como que el alumno entendió que todo el trabajo de instalación y configuración de las herramientas sería una fase previa al desarrollo del proyecto en sí, lo cuál no fue asi, puesto que más tarde se establecería que dicho trabajo formaría parte de una primera fase del proyecto: la prueba de concepto. Además, dicha incertidumbre dificultó la gestión del proyecto: la definición de las tareas, el control de los esfuerzos y un análisis adecuado desde el principio.

\item \textbf{Sistema operativo.} Si bien es cierto que el sistema operativo donde se desarrollase el proyecto no era un requisito, apareció desde el principio como un derivado de las herramientas a utilizar. Se propuso un sistema operativo \textit{Linux} \cite{wikilinux} sobre el que llevar a cabo la implementación de la solución. 
La propia instalación del sistema resultó inicialmente problemática en el portátil del alumno debido a la inexistencia de los \textit{drivers} de \textit{Linux} necesarios para la tarjeta gráfica \textit{Nvidia} \cite{wikinvidia} presente en el equipo, que resultaba en el no arranque del sistema. Tras unos días de consultas y búsquedas en páginas y foros de Internet, la solución al problema fue añadir la instrucción \textit{nouveau.modeset=0}, que desactiva los drivers libres de \textit{Nvidia} en el menú \textit{GRUB} \cite{grub} durante el arranque del sistema permitiendo que la gráfica que se ejecuta sea la otra presente en el equipo, la de \textit{Intel}.

\item \textbf{\textit{Hadoop}.} Como previamente se ha mencionado durante la prueba de concepto, la instalación de \textit{Hadoop} no fue la óptima desde el principio puesto que el alumno instaló una versión correspondiente a \textit{Ubuntu 14.04}, mientras que el sistema operativo instalado en el equipo era \textit{Ubuntu 16.04}. Debido a eso inicialmente \textit{Hadoop} dió problemas y en una fase posterior se tuvo que eliminar esta versión del equipo e instalar la correcta. Una vez solucionado ese problema, otro de los retos a los que se tuvo que enfrentar fue el entendimiento conceptual del sistema en sí. Se tuvieron que invertir horas en aprender a utilizar el sistema de ficheros \textit{HDFS}, algo necesario para el almacén de los datos de entrada de las diferentes fuentes. 
\par
Otro problema que surgió con \textit{Hadoop} fue debido a la falta de espacio en disco. Llegó un punto a lo largo de la duración del \textit{TFG} donde el disco duro del equipo del alumno se llenó y debido a eso las operaciones \gls{mapreduce} de \textit{Hadoop} se encolaban, se enmarcaban dentro de un estado de \textit{Pendiente} y nunca se ejecutaban. Pasaron varios días hasta que se llegó a la raíz del problema y, una vez liberada algo de memoria del disco duro, las operaciones \gls{mapreduce} de \textit{Hadoop} ya se podían realizar corréctamente. 

\item \textbf{\textit{Hive}.} Como ya se comentó, la instalación de \textit{Hive} en sí no presentó problemas. No obstante, el hecho de tratarse de un lenguaje \textit{SQL} nuevo, a pesar de su similitud con la sintáxis de \textit{MySQL} junto con las peculiaridades del sistema de archivos de \textit{Hadoop}, \textit{HDFS} sobre el que \textit{Hive} trabaja dificultaron un avance fluido del proyecto. En muchas ocasiones los datos de entrada daban problemas a la hora de importarlos en \textit{Hive}, por diferentes razones: inclusión de una cabecera que no debería aparecer en los datos de entrada, formateo incorrecto de los datos e incluso, por falta de experiencia, el uso incorrecto del delimitador en el lado de \textit{Hive}. Aparte de estas dificultades, \textit{Hive} también presentó problemas a la hora de intentar conectarlo diréctamente con \textit{JHipster}, tal como se verá en los siguientes apartados.

\item \textbf{Sqoop.} Al igual que \textit{Hive}, la instalación y configuración de \textit{Sqoop} no supuso un problema. No obstante, su manejo es lo que más dificultades presentó, puesto que al igual que los demás componentes, se trataba de una herramienta nueva para el alumno. Las tablas tanto de orígen (base de datos \textit{Hive} presente en \textit{HDFS}) como de destino (base de datos \textit{MySQL} de \textit{JHipster}) tenían que coincidir en estructura y tomó varios intentos hasta tener la configuración adecuada. 	

\item \textbf{\textit{JHipster}.} Con \textit{JHipster} se tuvo varios problemas, empezando por su instalación en el equipo. \textit{JHipster} utiliza \textit{Yarn}, \textit{Bower}, \textit{Node.js}, \textit{Gulp} y \textit{Yeoman} y, ya que el equipo tenía preinstaladas algunas de estas herramientas en sus antiguas versiones, al principio la instalación resultó en fallos que tomaron tiempo para solventar. Otro problema, una vez solucionado el anterior fue a la hora de la importación de una aplicación generada con \textit{JHipster} como proyecto dentro de \textit{IntelliJ}. \textit{Graddle} inicialmente no estaba corréctamente configurado en el equipo y por ello el proyecto era incapaz de descargar y configurar sus dependencias. Tras haber instalado \textit{Graddle} e importado el proyecto corréctamente, otro de los problemas encontrados apareció a la hora de importar un esquema de datos sobre \textit{JHipster} mediante \textit{JDL-Studio} \cite{jdlstudio}, el editor gráfico para creación de modelos de datos de \textit{JHipster}. Si bien la creación del propio modelo y su descarga en un fichero  \textit{.jh} no presentó dificultades, la importación del esquema dentro del proyecto mediante el comando \textit{jhipster import-jdl fichero.jh} provocaba errores en el proyecto en el momento de su ejecución. Gracias al control de versiones implementado mediante \textit{GIT} \cite{git} se pudo volver a la versión previa y desechar los cambios provocados por el comando anterior. Se descubrió que si las entidades se crean individualmente mediante el comando \textit{jhipster entity nombreEntidad} el fallo anterior ya no ocurre y se puede continuar con una ejecución correcta. 
\par 
Otro problema con \textit{JHipster} ocurrió al intentar actualizar la versión del mismo. Según las instrucciones de la página web, el proceso debería ser aparentemente sencillo. Se trata de ir a la localización de la aplicación creada con \textit{JHipster} y ejecutar el comando \textit{jhipster upgrade}. No obstante, dicho comando en ocasiones funcionaba, y en otras, tras esperar el tiempo de actualización, todo aparentaba normalidad hasta que al arrancar la aplicación aparecían errores referentes a determinados \textit{beans} de \textit{Spring} que \textit{JHipster} era incapaz de encontrar. A pesar de intentar solucionar dicho problema, en esos casos el alumno prefirió volver a la versión anterior del proyecto y desechar los cambios realizados por el comando \textit{upgrade}. Como \textit{JHipster} es un sistema en constante evolución, periódicamente los desarrolladores lanzan \textit{parches} mediante los que solucionan problemas como el descrito anteriormente. 

\item \textbf{\textit{Hive} y \textit{JHipster}.} Tal como se explica en la prueba de concepto (sección \ref{implementacion.prueba}), inicialmente la arquitectura del sistema debía reflejar una conexión dirécta entre la aplicación de \textit{JHipster} y la base de datos \textit{Hive}, mediante la sustitución de la base de datos de \textit{JHipster} (\textit{MySQL}) por \textit{Hive} en los ficheros de configuración del proyecto. Teniendo en mente que \textit{JHipster} diréctamente \textbf{no ofrece soporte} ni para \textit{Hive} ni para \textit{Hadoop}, la meta era \textit{hardcodear} todos los parámetros y conexiones necesarias para \textit{engañar} a la aplicación y conseguir que trabaje diréctamente con \textit{Hive}. Esto consistía en importar las librerías necesarias, crear las entidades de \textit{JHipster} dentro de \textit{Hive}, indicarle el driver \textit{JDBC} para conectarse con \textit{Hive} y mapear cada acceso a la base de datos previa \textit{MySQL} a la de \textit{Hive}. El primer gran obstáculo que se detectó fue la importación de las librerías. Resulta que al importar las librerías necesarias para \textit{Hive}, estas presentaban conflictos con las librerías que ya venían importadas por \textit{JHipster}. Encontrar los paquetes conflictivos llevó mucho tiempo, puesto que el árbol de dependencias del proyecto tenía un tamaño considerable y no se podía saber a priori cuál de los múltiples duplicados existentes fallaba. Como se verá a continuación, el problema de las librerías externas dentro de \textit{JHipster} seguirá apareciéndo con otros componentes, por lo que se puede entender que \textit{JHipster} no es la mejor elección cuando se quiera expandir el proyecto con muchas librerías externas o de terceros. Por otro lado, el problema de las entidades se abordó poco a poco, intentando migrar las tablas una a una. No obstante, \textit{JHipster} creaba dichas tablas con una sintáxis y unas propiedades y atributos acordes a \textit{MySQL}, algunos de los cuales eran inviables de construir en \textit{Hive} debido a la misma inexistencia de dicha funcionalidad. Las aplicaciones que \textit{JHipster} generan tienen una gran complejidad, con cientas de clases interoperando y compartiendo información, dificultando la depuración de la ejecución del programa y haciendo que una aproximación como la que se intentaba realizar en este período fuera inviable. Tras semanas de intentos frustrados, se consiguió que la  aplicación arrancara conectándose a \textit{Hive}, pero era incapaz de realizar cualquier función que se le pedía, como iniciar sesión con un usuario o registrar uno nuevo así que la arquitectura del sistema cambió, se desechó la idea de conectar diréctamente \textit{Hive} con \textit{JHipster} y se abordó una aproximación que involucrara un intermediario (\textit{Apache Sqoop}) entre \textit{Hive} y \textit{JHipster} y manteniendo la base de datos \textit{MySQL}.

\item \textbf{\textit{Pentaho Kettle}.} Aparte del problema anterior, el otro gran obstáculo que supuso un retraso del proyecto fue la elección de \textit{Pentaho Kettle} como sistema para realizar operaciones \textit{ETL} sobre los datos previo a su importación en \textit{Hadoop}. Aparentemente \textit{Pentaho Kettle} era la herramienta que se buscaba complementaria al \textit{Stack Tecnológico} del proyecto. No obstante, con el tiempo se observó que más que una ayuda resultó un inconveniente. Desde su instalación, que resultó conflictiva puesto que por alguna razón al descargar el programa en español, este se descargó con la mitad de sus componentes en inglés y la otra mitad en español hasta su editor gráfico, que no es exáctamente muy \textit{user-friendly}. Se tuvo problemas para conectarse a \textit{Hadoop} ya que, por culpa de una documentación pobre no se da a entender que antes de intentar acceder a \textit{Hadoop} mediante sus componentes se debía configurar un \textit{Cluster} de \textit{Hadoop} dentro del propio editor gráfico. Además se observó que el lanzamiento de un mismo proceso varias veces podía resultar en una ejecución exitosa o fallar estrepitósamente por razones aparentemente arbitrarias. 

\item \textbf{\textit{Pentaho Kettle} y \textit{JHipster}.} A la hora de integrar \textit{Pentaho Kettle} y \textit{JHipster}, al igual que ocurría con \textit{Hive}, el problema principal fueron las librerías necesarias para poder trabajar con \textit{Pentaho Kettle} dentro de la aplicación de \textit{JHipster}. En este caso, además, reincidiéndo en el problema de la existencia de una documentación pobre, no se listan las librerías necesarias para poder trabajar con \textit{Pentaho Kettle} desde un proyecto \textit{Java}. Por esa misma razón, si bien es cierto que para trabajos o procesos sencillos (de prueba) de \textit{Pentaho Kettle} se consiguió el pack de librerías necesarias a importar, en cuanto los procesos eran un poco más complejos (como los que realmente necesitaba el proyecto), las librerías empezaban a producir conflictos en el proyecto, resultando en la imposibilidad del arranque de la aplicación. El problema fue que conforme se solucionaban los errores, aparecían otros, y cada vez en más cantidad, haciéndo inviable la opción de \textit{Pentaho Kettle} para los requisitos que tenía el proyecto. Por ello, tras reiterados intentos de solucionar dichos problemas se optó por buscar otras alternativas y se encontró \textit{Talend} como herramienta definitiva.

\item \textbf{\textit{Talend}.} El único problema que presentó \textit{Talend} realmente no supuso un retraso tan considerable. \textit{Talend} autogenera código \textit{Java} conforme el usuario diseña sus procesos en el editor gráfico. Resulta que uno de sus componentes (\textit{tHDFSInput}) en una primera instalación fallaba pues al insertar el código \textit{Java} correspondiente, se dejaba por cerrar una llave, haciéndo que el programa diese error en tiempo de compilación y no se pudiese probar el proceso. No obstante, tras reinstalar \textit{Talend}, este problema cesó y todos sus componentes funcionaron acordes a su especificación. 

\item \textbf{\textit{Talend} y \textit{JHipster}.} Reapareciéndo por tercera vez, el problema de las librerías importadas en \textit{JHipster} volvió a surgir al intentar integrar el código generado por \textit{Talend} de uno de sus procesos en el proyecto de \textit{JHipster}. Habiéndo aprendido la lección de los anteriores intentos, esta vez no se reparó mucho en intentar solventar este problema sino que se adoptó una solución alternativa: crear un proyecto \textit{Java} nuevo que integrase todas las dependencias necesarias para el proceso de \textit{Talend} y encapsular todo su contenido en una librería ejecutable, fácilmente accesible desde \textit{JHipster}. Esta solución también se probó con \textit{Pentaho Kettle}, pero como ya se mencionó, debido a la falta de documentación acerca de las librerías a importar, se desistió en su resolución.
\end{itemize}




























