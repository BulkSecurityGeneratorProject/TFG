\chapter{Implementación}
\section{Prueba de concepto}
Explicar que una fase inicial del proyecto fue la prueba de concepto donde se demostró que las herramientas integradas funcionan en conjunto. Introducir brevemente el hecho de que inicialmente hubo problemas con algunas tecnologías, pero no entrar en detalles. Explicar las decisiones que se han tenido que tomar en resumidas cuentas.


\section{Prototipo real}



\par 
\textbf{Primera iteración para conseguir una integración y automatización completa - Productos autorizados de España}
\bigskip
\par Lo primero que se hizo fue implementar un simple proceso mediante la interfaz gráfica de Talend. Este proceso realiza las siguientes operaciones: 
\begin{enumerate}
\item Descarga desde la web del \textit{Mapama} el fichero excel de los productos fitosanitarios autorizados.
\item Convierte dicho excel a un formato openoffice para poder ser procesado desde Talend con los componentes excel correspondientes. 
\item Sube a Hadoop una versión sin procesar del fichero
\item Procesa el fichero añadiendole una columna llamada ID al principio y lo sube como versión procesada a Hadoop. 
\end{enumerate} 

A continuación se exportó el proceso desde Talend: 
\textit{Archivo $\rightarrow$ Export $\rightarrow$ Java $\rightarrow$ JAR file}. Esto exporta las clases y librerias que Talend necesita para lanzar el job en un archivo comprimido llamado <nombre\_job>.jar
El siguiente paso fue descomprimir el JAR en cuestión, analizar su contenido y ver cómo se podría importar en un proyecto Java. El JAR contenía varias carpetas y ficheros pero lo que interesa es lo siguiente:
\bigskip
\par 
\dirtree{%
.1 Nombre\_del\_jar.
.2 lib.
.3 \textit{librerias jar}.
.2 Nombre\_del\_proyecto.
.3 Nombre\_del\_job.
.4 \textit{clase java principal del job}.
.2 routines.
.3 system.
.4 api.
.5 \textit{clases java}.
.4 xml.
.5 sax.
.6 \textit{clases java}.
.4 \textit{clases java}.
.3 \textit{clases java}.
}
\bigskip
\par
Así pues, a continuación se creó un nuevo proyecto Java con IntelliJ y Maven (TalendCrawler) y se copiaron todas las clases Java con su correspondiente estructura de carpetas. Dentro del fichero pom.xml del proyecto TalendCrawler donde se importaron todas las dependencias de Talend que figuraban como librerías locales en la carpeta lib. Para ello se tuvo que definir el \textit{repositorio de Cloudera} \cite{cloudera}, que es desde donde Maven buscaría la mayoría de librerías. Tras comprobar que la aplicación arrancaba y se comportaba correctamente, el próximo paso fue encapsular y exportar la aplicación como un Jar, en conjunto con sus librerías. Para ello se hizo uso del plugin \textit{one-jar} de Maven que recoge las dependencias del proyecto y las empaqueta junto a las otras clases en un único jar.\par 

En el proyecto de JHipster lo que se hizo fue crear una clase llamada Talend, desde la que periódicamente (mediante @Scheduled) se ejecutaba el Jar anterior a través del comando Runnable. 

\par
Teniendo ya el proceso de \textit{Talend} integrado en la aplicación de \textit{JHipster}, el siguiente problema a abordar fue el de la automatización de su ejecución. Se sabe que los productos fitosanitarios autorizados son actualizados periódicamente en la web de \textit{Mapama}. Por eso mismo, nuestra aplicación requería también de una descarga periódica de dichos datos, para asegurarse de que en todo momento el programa tiene la versión actualizada de los fitosanitarios autorizados de España. Esto se consiguió gracias al \textit{módulo de scheduling}\cite{spring_scheduling} de \textit{Spring} que permite programar la ejecución de un método de manera periódica. Como decisión estratégica se propuso lanzar el proceso de \textit{Talend} cada media hora. Resuelto este problema también, el siguiente objetivo fue automatizar toda la ejecución del proceso, desde la descarga del fichero de los productos autorizados hasta la visualización de los datos mediante \textit{JHipster}. Aprovechandose del mismo módulo anterior de scheduling, el desarrollo tendría que seguir el siguiente esquema: 
\begin{itemize}
\item Primero, los datos deberían descargarse y procesarse y almacenarse en \textit{Hadoop} mediante el módulo de \textit{Talend}.
\item A continuación, se debería implementar otro módulo encargado de la carga de dichos datos procesados a una tabla de \textit{Hive}.
\item Después de eso, se deberían transferir los datos de \textit{Hive} a la base de datos \textit{MySQL} que emplea \textit{JHipster}.
\end{itemize}
  \par Así pues, para cada uno de los módulos mencionados se creó un paquete con una clase que contenía los métodos necesarios para lograr sus tareas particulares. A continuación se adjunta un diagrama de clases para ilustrar de una mejor forma la infraestructura que se construyó para soportar el comportamiento mencionado en los puntos anteriores.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Imagenes/Paquete_processes}
    \caption{Diagrama de clases y paquetes para soportar la automatización del \textit{workflow}}
    \label{fig:diag_clases}
\end{figure}

\par Una vez vista la estructura del diagrama anterior, a continuación se presenta un diagrama de secuencia para ilustrar la interacción de los diferentes componentes y el rol que juegan en el \textit{workflow} desde que los datos se descargan hasta que pasan a visualizarse mediante \textit{JHipster}.

\begin{landscape}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Imagenes/processes_sequence}
    \caption{Diagrama de secuencia del \textit{workflow} implementado}
    \label{fig:diag_clases}
\end{figure}

\end{landscape}
\bigskip



\par 
\textbf{Segunda iteración para conseguir una integración y automatización completa - Sustancias activas de Europa}
\bigskip
\par 
La primera iteración supuso los mayores problemas debido no solo al desconocimiento previo de las tecnologías sino también al hecho de no saber exactamente si dichas tecnologías iban a funcionar en conjunto. Una vez conocidas las tecnologías y tomado un primer contacto con ellas (el alumno no había trabajado con \textit{Talend} previamente) la segunda parte de la integración se llevó a cabo de una manera mucho más fluída. Para esta iteración se conocía previamente el \textit{modus operandi} para automatizar todo el proceso, desde la descarga de los datos hasta su visualización con \textit{JHipster}. Por lo tanto, lo único diferente con respecto a la primera iteración fue desarrollar el trabajo de procesado específico de los datos de entrada. 
\par  
Para la segunda iteración se eligieron los datos expuestos en la \textit{Base de datos europea sobre pesticidas} \cite{pesticides_eu} para seguir expandiendo la solución. Como ya se ha explicado, el objetivo de este proyecto es conseguir validar un modelo de integración para datos sobre productos fitosanitarios. En la primera iteración se obtuvieron los datos sobre los productos fitosanitarios autorizados en España. Estos contenían un campo llamado \textit{Formulado}. Dicho campo se refiere a la \textit{sustancia activa} de cada producto. Resulta que los datos descargados de la \textit{base de datos europea} contienen una amplia estructura de datos e información relativa a los productos fitosanitarios. No obstante, dicha cantidad de información también resulta excesiva. Por ello, se ha optado por una aproximación minuciosa, cogiendo y procesando un solo elemento de todos los disponibles a la vez. En este caso dicho elemento corresponde a un fichero con la información relativa a las \textit{sustancias activas}. Esta aproximación permitire ese objetivo de integración puesto que gracias a ello se puedo hacer un mapeo casi directo con los datos sobre productos autorizados de España. 
\par 
De igual manera que en la primera iteración, se implementó en \textit{Talend} el workflow necesario para procesar los datos de las sustancias activas. Esto es, por una parte, descargarlos de la página web, añadir la fecha y hora del momento de la descarga y guardarlos en \textit{Hadoop} como datos en crudo de España sin alterar ni su formato ni su contenido. Por otra parte se formateó el contenido, para almacenar en \textit{Hadoop} un fichero \textit{.csv} con sólamente la información relevante del fichero original y con una columna extra para el identificador de las filas. El mismo proceso de Talend también se encarga de subir este \textit{.csv} a Hadoop en la carpeta de datos procesados de Europa. 
\par
A continuación se preparó la infraestructura necesaria para soportar la carga de datos en \textit{Hive} mediante una nueva tabla que se mantendrá actualizada con los datos más recientes sobre sustancias activas de Europa. Esto se consiguió gracias al desarrollo implementado en el proyecto de \textit{JHipster} desde el que periódicamente se lanza el workflow anterior de Talend, y posteriormente se realiza una importación de los datos a \textit{Hive}. Además, en el lado del cliente, en \textit{JHipster} se creó la tabla correspondiente a la d \textit{Hive} en \textit{MySQL} y, una vez más, periódicamente, los datos de \textit{Hive} son transferidos a la base de datos \textit{MySQL} a través de \textit{Sqoop}. El resultado de esta iteración es que periódicamente, en \textit{JHipster} se pueden visualizar los datos actualizados de las sustancias activas europeas sin necesidad de que el usuario tenga que intervenir o interactuar con el sistema en ningún momento. 


\bigskip
\par 
\textbf{Fichero de configuración}
\par
Para simplificar el acceso a los recursos se ha hecho uso de un fichero de configuración a los que acceden varios componentes: En primer lugar, el script bash que descarga los datos de los productos autorizados del \textit{Mapama} \cite{mapama}. Este Script usa una función bash para solicitar los valores del fichero de propiedades de la web del \textit{Mapama}, y saber la ruta en el sistema donde guardar dicho fichero. Si en cualquier momento se quiere modificar dicha localización, gracias al fichero de configuración, el único sitio que se debería modificar sería en el propio fichero. 
\par En segundo lugar, la aplicación Java del Job de Talend también accede a dicho fichero de configuración, puesto que en él se han establecido tanto rutas de almacenamiento dentro del HDFS de Hadoop, como el nombre del nodo o del usuario. No obstante, tal como se ha comentado en el apartado anterior, esta aplicación Java ha tenido que ser empaquetada en un Jar único y conjunto con todas sus librerías. Entonces ... ¿cómo accede a dicho fichero de configuración?. La solución ha sido hacer que el Jar reciba la ruta a dicho fichero mediante un argumento, de forma robusta, tal que si no recibe argumentos, o si el fichero que se le pasa no es un fichero de propiedades, el proceso alerta del error y se detiene. 
\bigskip
\par
\textit{Explicar en detalle el funcionamiento típico de la aplicación y la presentación de los datos. Hacer una especie de manual del usuario resumido y si hay algún aspecto interesante de la aplicación, hacer hincapié en el.}
\section{Problemas técnicos detectados}
Mencionar los problemas confrontados durante la realización del proyecto y la manera en la que se han solucionado.
